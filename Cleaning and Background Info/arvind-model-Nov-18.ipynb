{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In this notebook, we will use xgboost to train a binary classification model to predict y given the course data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/final_dataset_Nov_12.csv\")\n",
    "courses = ['101',\n",
    " '104',\n",
    " '105',\n",
    " '140',\n",
    " '143',\n",
    " '145',\n",
    " '150',\n",
    " '151',\n",
    " '160',\n",
    " '165',\n",
    " '166',\n",
    " '201',\n",
    " '207',\n",
    " '240',\n",
    " '265',\n",
    " '266',\n",
    " '267',\n",
    " '301',\n",
    " '302',\n",
    " '304',\n",
    " '314',\n",
    " '317',\n",
    " '341',\n",
    " '342',\n",
    " '350',\n",
    " '365',\n",
    " '373',\n",
    " '385',\n",
    " '397',\n",
    " '414',\n",
    " '415',\n",
    " '435',\n",
    " '436',\n",
    " '497']\n",
    "\n",
    "#loop through the semesters\n",
    "for i in range(1, 9):\n",
    "    # Create a new column for the semester\n",
    "    df[f'SEM_{i}'] = df[courses].apply(lambda x: x.tolist().count(i) - x.tolist().count(-i), axis=1)\n",
    "\n",
    "# Create cumulative sum columns for each semester. This will give us the total number of courses passed by the student up to that semester. In particular, the final column will give us the total number of courses passed by the student within 8 semesters.\n",
    "df[[f'SEM_{i}_cumulative' for i in range(1, 9)]] = df[[f'SEM_{i}' for i in range(1, 9)]].cumsum(axis=1)\n",
    "\n",
    "\n",
    "#define X to be df without the y column and STUDENT column\n",
    "X = df.drop(columns=['y', 'STUDENT','GRAD_SEM','SEM_1_cumulative'])\n",
    "y=df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.6, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.2, 'lambda': 1.5, 'gamma': 0, 'eval_metric': 'logloss', 'colsample_bytree': 0.8, 'alpha': 0.2}\n",
      "Best accuracy found:  0.6553189160915042\n"
     ]
    }
   ],
   "source": [
    "#output for sem_i and sem_i_cumulative\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=500, \n",
    "                                   scoring='accuracy', \n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy found: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 1.0, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 6, 'learning_rate': 0.2, 'lambda': 1.5, 'gamma': 0.2, 'eval_metric': 'logloss', 'colsample_bytree': 0.8, 'alpha': 0.1}\n",
      "Best accuracy found:  0.6579069154905282\n"
     ]
    }
   ],
   "source": [
    "#output for sem_i\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=500, \n",
    "                                   scoring='accuracy', \n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1)\n",
    "random_search.fit(X_train.drop(columns=[f'SEM_{i}_cumulative' for i in range(2,9)]), y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy found: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_objective</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.196911</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.004607</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'subsample': 0.6, 'objective': 'binary:logist...</td>\n",
       "      <td>0.633442</td>\n",
       "      <td>0.626906</td>\n",
       "      <td>0.606209</td>\n",
       "      <td>0.629085</td>\n",
       "      <td>0.627248</td>\n",
       "      <td>0.624578</td>\n",
       "      <td>0.009475</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111196</td>\n",
       "      <td>0.018554</td>\n",
       "      <td>0.014844</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'subsample': 1.0, 'objective': 'binary:logist...</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.655773</td>\n",
       "      <td>0.635621</td>\n",
       "      <td>0.644880</td>\n",
       "      <td>0.662670</td>\n",
       "      <td>0.651162</td>\n",
       "      <td>0.009666</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111486</td>\n",
       "      <td>0.008059</td>\n",
       "      <td>0.017822</td>\n",
       "      <td>0.004035</td>\n",
       "      <td>0.8</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'subsample': 0.8, 'objective': 'binary:logist...</td>\n",
       "      <td>0.660131</td>\n",
       "      <td>0.653050</td>\n",
       "      <td>0.631264</td>\n",
       "      <td>0.649782</td>\n",
       "      <td>0.657221</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119280</td>\n",
       "      <td>0.015534</td>\n",
       "      <td>0.015074</td>\n",
       "      <td>0.004758</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'subsample': 0.6, 'objective': 'binary:logist...</td>\n",
       "      <td>0.662309</td>\n",
       "      <td>0.648693</td>\n",
       "      <td>0.635621</td>\n",
       "      <td>0.642702</td>\n",
       "      <td>0.667030</td>\n",
       "      <td>0.651271</td>\n",
       "      <td>0.011792</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094043</td>\n",
       "      <td>0.014125</td>\n",
       "      <td>0.017434</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'subsample': 1.0, 'objective': 'binary:logist...</td>\n",
       "      <td>0.655229</td>\n",
       "      <td>0.654684</td>\n",
       "      <td>0.633442</td>\n",
       "      <td>0.631808</td>\n",
       "      <td>0.661580</td>\n",
       "      <td>0.647349</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.120761</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>0.013744</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.8</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'subsample': 0.8, 'objective': 'binary:logist...</td>\n",
       "      <td>0.656318</td>\n",
       "      <td>0.652505</td>\n",
       "      <td>0.630719</td>\n",
       "      <td>0.626906</td>\n",
       "      <td>0.660490</td>\n",
       "      <td>0.645388</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.129776</td>\n",
       "      <td>0.024262</td>\n",
       "      <td>0.018041</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'subsample': 1.0, 'objective': 'binary:logist...</td>\n",
       "      <td>0.592048</td>\n",
       "      <td>0.592048</td>\n",
       "      <td>0.576797</td>\n",
       "      <td>0.596405</td>\n",
       "      <td>0.593460</td>\n",
       "      <td>0.590152</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.233304</td>\n",
       "      <td>0.071253</td>\n",
       "      <td>0.055380</td>\n",
       "      <td>0.051711</td>\n",
       "      <td>0.8</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'subsample': 0.8, 'objective': 'binary:logist...</td>\n",
       "      <td>0.632898</td>\n",
       "      <td>0.625272</td>\n",
       "      <td>0.594771</td>\n",
       "      <td>0.628540</td>\n",
       "      <td>0.620163</td>\n",
       "      <td>0.620329</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.364114</td>\n",
       "      <td>0.054710</td>\n",
       "      <td>0.075608</td>\n",
       "      <td>0.079793</td>\n",
       "      <td>0.6</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'subsample': 0.6, 'objective': 'binary:logist...</td>\n",
       "      <td>0.651416</td>\n",
       "      <td>0.643791</td>\n",
       "      <td>0.635621</td>\n",
       "      <td>0.635621</td>\n",
       "      <td>0.656676</td>\n",
       "      <td>0.644625</td>\n",
       "      <td>0.008416</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.162897</td>\n",
       "      <td>0.070550</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.8</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'subsample': 0.8, 'objective': 'binary:logist...</td>\n",
       "      <td>0.655773</td>\n",
       "      <td>0.651416</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.640523</td>\n",
       "      <td>0.663215</td>\n",
       "      <td>0.648656</td>\n",
       "      <td>0.010974</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0         0.196911      0.011140         0.014549        0.004607   \n",
       "1         0.111196      0.018554         0.014844        0.005814   \n",
       "2         0.111486      0.008059         0.017822        0.004035   \n",
       "3         0.119280      0.015534         0.015074        0.004758   \n",
       "4         0.094043      0.014125         0.017434        0.006286   \n",
       "..             ...           ...              ...             ...   \n",
       "495       0.120761      0.009861         0.013744        0.001994   \n",
       "496       0.129776      0.024262         0.018041        0.006775   \n",
       "497       0.233304      0.071253         0.055380        0.051711   \n",
       "498       0.364114      0.054710         0.075608        0.079793   \n",
       "499       0.162897      0.070550         0.009214        0.000160   \n",
       "\n",
       "     param_subsample  param_objective  param_n_estimators  \\\n",
       "0                0.6  binary:logistic                 100   \n",
       "1                1.0  binary:logistic                 100   \n",
       "2                0.8  binary:logistic                 100   \n",
       "3                0.6  binary:logistic                 100   \n",
       "4                1.0  binary:logistic                 100   \n",
       "..               ...              ...                 ...   \n",
       "495              0.8  binary:logistic                 100   \n",
       "496              1.0  binary:logistic                 100   \n",
       "497              0.8  binary:logistic                 100   \n",
       "498              0.6  binary:logistic                 100   \n",
       "499              0.8  binary:logistic                 100   \n",
       "\n",
       "     param_min_child_weight  param_max_depth  param_learning_rate  ...  \\\n",
       "0                         3                6                 0.01  ...   \n",
       "1                         5                3                 0.20  ...   \n",
       "2                         3                6                 0.20  ...   \n",
       "3                         5                6                 0.10  ...   \n",
       "4                         3                3                 0.10  ...   \n",
       "..                      ...              ...                  ...  ...   \n",
       "495                       5                3                 0.10  ...   \n",
       "496                       3                3                 0.01  ...   \n",
       "497                       3                6                 0.01  ...   \n",
       "498                       1               10                 0.10  ...   \n",
       "499                       5                3                 0.10  ...   \n",
       "\n",
       "     param_alpha                                             params  \\\n",
       "0            0.0  {'subsample': 0.6, 'objective': 'binary:logist...   \n",
       "1            0.2  {'subsample': 1.0, 'objective': 'binary:logist...   \n",
       "2            0.1  {'subsample': 0.8, 'objective': 'binary:logist...   \n",
       "3            0.1  {'subsample': 0.6, 'objective': 'binary:logist...   \n",
       "4            0.0  {'subsample': 1.0, 'objective': 'binary:logist...   \n",
       "..           ...                                                ...   \n",
       "495          0.0  {'subsample': 0.8, 'objective': 'binary:logist...   \n",
       "496          0.1  {'subsample': 1.0, 'objective': 'binary:logist...   \n",
       "497          0.0  {'subsample': 0.8, 'objective': 'binary:logist...   \n",
       "498          0.2  {'subsample': 0.6, 'objective': 'binary:logist...   \n",
       "499          0.2  {'subsample': 0.8, 'objective': 'binary:logist...   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score split3_test_score  \\\n",
       "0            0.633442           0.626906           0.606209          0.629085   \n",
       "1            0.656863           0.655773           0.635621          0.644880   \n",
       "2            0.660131           0.653050           0.631264          0.649782   \n",
       "3            0.662309           0.648693           0.635621          0.642702   \n",
       "4            0.655229           0.654684           0.633442          0.631808   \n",
       "..                ...                ...                ...               ...   \n",
       "495          0.656318           0.652505           0.630719          0.626906   \n",
       "496          0.592048           0.592048           0.576797          0.596405   \n",
       "497          0.632898           0.625272           0.594771          0.628540   \n",
       "498          0.651416           0.643791           0.635621          0.635621   \n",
       "499          0.655773           0.651416           0.632353          0.640523   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0             0.627248         0.624578        0.009475              396  \n",
       "1             0.662670         0.651162        0.009666               86  \n",
       "2             0.657221         0.650289        0.010146              119  \n",
       "3             0.667030         0.651271        0.011792               80  \n",
       "4             0.661580         0.647349        0.012275              260  \n",
       "..                 ...              ...             ...              ...  \n",
       "495           0.660490         0.645388        0.013820              309  \n",
       "496           0.593460         0.590152        0.006864              492  \n",
       "497           0.620163         0.620329        0.013439              437  \n",
       "498           0.656676         0.644625        0.008416              315  \n",
       "499           0.663215         0.648656        0.010974              197  \n",
       "\n",
       "[500 rows x 24 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Best parameters found:  {'subsample': 0.6, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 3, 'max_depth': 6, 'learning_rate': 0.1, 'lambda': 2, 'gamma': 0, 'eval_metric': 'logloss', 'colsample_bytree': 1.0, 'alpha': 0.1}\n",
      "Best accuracy found:  0.6553193798075764\n"
     ]
    }
   ],
   "source": [
    "#output for sem_i_cumulative\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=500, \n",
    "                                   scoring='accuracy', \n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1)\n",
    "random_search.fit(X_train.drop(columns=[f'SEM_{i}' for i in range(2,9)]), y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy found: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/w7q1fjwd5kn8xvqty5sp3h7r0000gn/T/ipykernel_21628/4171480707.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Define your parameters without 'n_estimators'\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    # other parameters\n",
    "}\n",
    "\n",
    "# Specify the number of boosting rounds\n",
    "num_boost_round = 200\n",
    "\n",
    "# Initialize KFold and DataFrame to store metrics\n",
    "skf = StratifiedKFold(n_splits=5, \n",
    "           shuffle=True, \n",
    "           random_state=42)\n",
    "metrics_df = pd.DataFrame(columns=['cv_round', 'boost_round', 'accuracy', 'precision', 'recall', 'f1_score'])\n",
    "\n",
    "# Perform manual cross-validation\n",
    "for cv_round, (train_index, test_index) in enumerate(skf.split(X_train,y_train)):\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    dtrain_cv = xgb.DMatrix(data=X_train_cv, label=y_train_cv)\n",
    "    dtest_cv = xgb.DMatrix(data=X_test_cv, label=y_test_cv)\n",
    "    \n",
    "    # Initialize model to None\n",
    "    model = None\n",
    "    \n",
    "    # Train the model for each boosting round\n",
    "    for boost_round in range(1, num_boost_round + 1):\n",
    "        model = xgb.train(params, dtrain_cv, num_boost_round=1, xgb_model=model)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_prob = model.predict(dtest_cv)\n",
    "        y_pred = [1 if prob >= 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "        # Calculate scoring metrics\n",
    "        accuracy = accuracy_score(y_test_cv, y_pred)\n",
    "        precision = precision_score(y_test_cv, y_pred)\n",
    "        recall = recall_score(y_test_cv, y_pred)\n",
    "        f1 = f1_score(y_test_cv, y_pred)\n",
    "        \n",
    "        # Store the metrics in the DataFrame\n",
    "        new_row = pd.DataFrame([{\n",
    "            'cv_round': cv_round + 1,\n",
    "            'boost_round': boost_round,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }])\n",
    "        metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "\n",
    "# Set multi-level index\n",
    "metrics_df.set_index(['cv_round', 'boost_round'], inplace=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "# print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             accuracy  precision    recall  f1_score\n",
      "boost_round                                         \n",
      "1            0.553998   0.547863  0.993438  0.706219\n",
      "2            0.564483   0.554298  0.984857  0.709327\n",
      "3            0.566661   0.556890  0.964918  0.706134\n",
      "4            0.590897   0.600320  0.816535  0.677219\n",
      "5            0.625219   0.649177  0.692614  0.663383\n",
      "...               ...        ...       ...       ...\n",
      "196          0.654637   0.684682  0.667082  0.675670\n",
      "197          0.654637   0.684681  0.667082  0.675671\n",
      "198          0.654365   0.684246  0.667335  0.675580\n",
      "199          0.654501   0.684327  0.667587  0.675752\n",
      "200          0.654365   0.684152  0.667587  0.675666\n",
      "\n",
      "[200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#for each boosting round, calculate the average of each metric\n",
    "avg_metrics = metrics_df.groupby('boost_round').mean()\n",
    "\n",
    "# Print the average metrics\n",
    "print(avg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6561 candidates, totalling 32805 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Perform grid search without early stopping\u001b[39;00m\n\u001b[1;32m     23\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     24\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mxgb_model, \n\u001b[1;32m     25\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Print the best parameters and best score\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1014\u001b[0m     )\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     )\n\u001b[0;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [200],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform grid search without early stopping\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=5, \n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(\n",
    "    X_train, \n",
    "    y_train\n",
    ")\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best accuracy found: \", grid_search.best_score_)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best accuracy found: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6449\n",
      "Confusion Matrix:\n",
      "[[548 297]\n",
      " [355 636]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.65      0.63       845\n",
      "           1       0.68      0.64      0.66       991\n",
      "\n",
      "    accuracy                           0.64      1836\n",
      "   macro avg       0.64      0.65      0.64      1836\n",
      "weighted avg       0.65      0.64      0.65      1836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your parameters without 'n_estimators'\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    # other parameters\n",
    "}\n",
    "\n",
    "# Specify the number of boosting rounds\n",
    "num_boost_round = 100\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "# Create the DMatrix for the test data\n",
    "dtest = xgb.DMatrix(data=X_test)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(y_test, y_pred_binary)\n",
    "print('Classification Report:')\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
