{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data handling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STUDENT</th>\n",
       "      <th>GRAD_SEM</th>\n",
       "      <th>101</th>\n",
       "      <th>101_SEM</th>\n",
       "      <th>104</th>\n",
       "      <th>104_SEM</th>\n",
       "      <th>105</th>\n",
       "      <th>105_SEM</th>\n",
       "      <th>140</th>\n",
       "      <th>140_SEM</th>\n",
       "      <th>...</th>\n",
       "      <th>414_SEM</th>\n",
       "      <th>415</th>\n",
       "      <th>415_SEM</th>\n",
       "      <th>435</th>\n",
       "      <th>435_SEM</th>\n",
       "      <th>436</th>\n",
       "      <th>436_SEM</th>\n",
       "      <th>497</th>\n",
       "      <th>497_SEM</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9174</th>\n",
       "      <td>13059</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9175</th>\n",
       "      <td>13060</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9176</th>\n",
       "      <td>13061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9177</th>\n",
       "      <td>13062</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9178</th>\n",
       "      <td>13063</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9179 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      STUDENT  GRAD_SEM  101  101_SEM  104  104_SEM  105  105_SEM  140  \\\n",
       "0           0       8.0    0        0    0        0    0        0    0   \n",
       "1           3       0.0    0        0    0        0    0        0    0   \n",
       "2           4       8.0    0        0    0        0    0        0    0   \n",
       "3           5       0.0    0        0    0        0    0        0    1   \n",
       "4           7       7.0    0        0    1        4    0        0    0   \n",
       "...       ...       ...  ...      ...  ...      ...  ...      ...  ...   \n",
       "9174    13059       4.0    0        0    0        0    0        0    1   \n",
       "9175    13060      10.0    0        0    0        0    0        0    0   \n",
       "9176    13061       0.0    0        0    0        0    0        0    0   \n",
       "9177    13062       5.0    0        0    0        0    0        0    0   \n",
       "9178    13063       9.0    0        0    0        0    0        0    0   \n",
       "\n",
       "      140_SEM  ...  414_SEM  415  415_SEM  435  435_SEM  436  436_SEM  497  \\\n",
       "0           0  ...        0    0        0    0        0    0        0    0   \n",
       "1           0  ...        0    0        0    0        0    0        0    0   \n",
       "2           0  ...        0    0        0    0        0    0        0    0   \n",
       "3           2  ...        0    0        0    0        0    0        0    0   \n",
       "4           0  ...        0    0        0    0        0    0        0    0   \n",
       "...       ...  ...      ...  ...      ...  ...      ...  ...      ...  ...   \n",
       "9174        3  ...        0    0        0    0        0    0        0    0   \n",
       "9175        1  ...        0    0        0    0        0    0        0    0   \n",
       "9176        0  ...        0    0        0    0        0    0        0    0   \n",
       "9177        0  ...        0    0        0    0        0    0        0    0   \n",
       "9178        0  ...        0    0        0    0        0    0        0    0   \n",
       "\n",
       "      497_SEM  y  \n",
       "0           0  1  \n",
       "1           0  0  \n",
       "2           0  1  \n",
       "3           0  0  \n",
       "4           0  1  \n",
       "...       ... ..  \n",
       "9174        0  1  \n",
       "9175        0  0  \n",
       "9176        0  0  \n",
       "9177        0  1  \n",
       "9178        0  0  \n",
       "\n",
       "[9179 rows x 71 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the main dataset\n",
    "df = pd.read_csv('../Data/final_dataset_Nov_9.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = ['101',\n",
    " '104',\n",
    " '105',\n",
    " '140',\n",
    " '143',\n",
    " '145',\n",
    " '150',\n",
    " '151',\n",
    " '160',\n",
    " '165',\n",
    " '166',\n",
    " '201',\n",
    " '207',\n",
    " '240',\n",
    " '265',\n",
    " '266',\n",
    " '267',\n",
    " '301',\n",
    " '302',\n",
    " '304',\n",
    " '314',\n",
    " '317',\n",
    " '341',\n",
    " '342',\n",
    " '350',\n",
    " '365',\n",
    " '373',\n",
    " '385',\n",
    " '397',\n",
    " '414',\n",
    " '415',\n",
    " '435',\n",
    " '436',\n",
    " '497']\n",
    "\n",
    "course_SEM = [f'{x}_SEM' for x in courses]\n",
    "\n",
    "X = df.drop(columns=['y','GRAD_SEM','STUDENT'])\n",
    "y= df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:972: RuntimeWarning: divide by zero encountered in power\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:972: RuntimeWarning: invalid value encountered in multiply\n",
      "  X2 = np.dot(Xm, R * (S ** (-0.5)))\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:975: RuntimeWarning: divide by zero encountered in log\n",
      "  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda: Mean accuracy = 0.5800, Std = 0.0144\n",
      "log_reg: Mean accuracy = 0.5799, Std = 0.0143\n",
      "svc_linear: Mean accuracy = 0.5800, Std = 0.0144\n",
      "qda: Mean accuracy = 0.5140, Std = 0.0275\n",
      "lda_poly: Mean accuracy = 0.5957, Std = 0.0187\n",
      "log_reg_poly: Mean accuracy = 0.5951, Std = 0.0192\n",
      "gnb: Mean accuracy = 0.5247, Std = 0.0081\n",
      "knn: Mean accuracy = 0.5392, Std = 0.0329\n",
      "svc_rbf: Mean accuracy = 0.5808, Std = 0.0152\n",
      "random_forest: Mean accuracy = 0.5792, Std = 0.0146\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\"\"\"\n",
    "This script performs feature selection and model evaluation using various classifiers with a pipeline approach.\n",
    "\n",
    "The script includes the following steps:\n",
    "1. Import necessary libraries and modules.\n",
    "2. Define the number of features to select using SelectKBest.\n",
    "3. Create a dictionary of classifiers, each with a pipeline that includes feature selection and the classifier.\n",
    "4. Perform 5-fold cross-validation for each model.\n",
    "5. Print the cross-validation results, including mean accuracy and standard deviation for each model.\n",
    "\n",
    "Classifiers included:\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Classifier (SVC)\n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "- Polynomial LDA\n",
    "- Polynomial Logistic Regression\n",
    "- Gaussian Naive Bayes (GNB)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Radial Basis Function (RBF) SVC\n",
    "- Random Forest\n",
    "\n",
    "Parameters:\n",
    "- k (int): The number of features to select using SelectKBest.\n",
    "\n",
    "Possible values for the parameter \"scoring\" in cross_val_score:\n",
    "- 'accuracy'\n",
    "- 'adjusted_rand_score'\n",
    "- 'average_precision'\n",
    "- 'balanced_accuracy'\n",
    "- 'brier_score_loss'\n",
    "- 'f1'\n",
    "- 'f1_micro'\n",
    "- 'f1_macro'\n",
    "- 'f1_weighted'\n",
    "- 'f1_samples'\n",
    "- 'neg_log_loss'\n",
    "- 'precision'\n",
    "- 'recall'\n",
    "- 'roc_auc'\n",
    "- 'roc_auc_ovr'\n",
    "- 'roc_auc_ovo'\n",
    "- 'roc_auc_ovr_weighted'\n",
    "- 'roc_auc_ovo_weighted'\n",
    "- 'neg_mean_absolute_error'\n",
    "- 'neg_mean_squared_error'\n",
    "- 'neg_root_mean_squared_error'\n",
    "- 'r2'\n",
    "- 'explained_variance'\n",
    "- 'max_error'\n",
    "- 'neg_median_absolute_error'\n",
    "- 'neg_mean_poisson_deviance'\n",
    "- 'neg_mean_gamma_deviance'\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the number of features to select\n",
    "k = 10\n",
    "\n",
    "\n",
    "# Update classifiers to include feature selection\n",
    "classifiers = {\n",
    "    'lda': Pipeline([('select', SelectKBest(f_classif, k=k)), ('lda', LinearDiscriminantAnalysis())]),\n",
    "    'log_reg': Pipeline([('select', SelectKBest(f_classif, k=k)), ('log_reg', LogisticRegression(max_iter=100000, random_state=42))]),\n",
    "    'svc_linear': Pipeline([('select', SelectKBest(f_classif, k=k)), ('svc_linear', LinearSVC(dual='auto'))]),\n",
    "    'qda': Pipeline([('select', SelectKBest(f_classif, k=k)), ('qda', QuadraticDiscriminantAnalysis())]),\n",
    "    'lda_poly': Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures(2)), ('select', SelectKBest(f_classif, k=k)), ('lda', LinearDiscriminantAnalysis())]),\n",
    "    'log_reg_poly': Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures(2)), ('select', SelectKBest(f_classif, k=k)), ('log_reg', LogisticRegression(penalty=None, max_iter=100000))]),\n",
    "    'gnb': Pipeline([('select', SelectKBest(f_classif, k=k)), ('gnb', GaussianNB())]),\n",
    "    'knn': Pipeline([('scale', StandardScaler()), ('select', SelectKBest(f_classif, k=k)), ('knn', KNeighborsClassifier())]),\n",
    "    'svc_rbf': Pipeline([('scale', StandardScaler()), ('select', SelectKBest(f_classif, k=k)), ('svc', SVC(kernel='rbf'))]),\n",
    "    'random_forest': Pipeline([('select', SelectKBest(f_classif, k=k)), ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation for each model\n",
    "cv_results = {}\n",
    "for model_name, model in classifiers.items():\n",
    "    scores = cross_val_score(model, \n",
    "                             X_train[courses], \n",
    "                             y_train, \n",
    "                             cv=5,\n",
    "                             scoring='accuracy',\n",
    "                             verbose=0,\n",
    "                             n_jobs=-1)\n",
    "    cv_results[model_name] = scores\n",
    "    # print(f\"{model_name}: Mean -logloss = {scores.mean():.4f}, Std = {scores.std():.4f}\")\n",
    "\n",
    "# Print the cross-validation results\n",
    "for model_name, scores in cv_results.items():\n",
    "    print(f\"{model_name}: Mean accuracy = {scores.mean():.4f}, Std = {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda: Mean accuracy = 0.5921, Std = 0.0060\n",
      "log_reg: Mean accuracy = 0.5920, Std = 0.0069\n",
      "svc_linear: Mean accuracy = 0.5921, Std = 0.0059\n",
      "qda: Mean accuracy = 0.5313, Std = 0.0158\n",
      "lda_poly: Mean accuracy = 0.5830, Std = 0.0099\n",
      "log_reg_poly: Mean accuracy = 0.5827, Std = 0.0100\n",
      "gnb: Mean accuracy = 0.5314, Std = 0.0151\n",
      "knn: Mean accuracy = 0.5746, Std = 0.0115\n",
      "svc_rbf: Mean accuracy = 0.5951, Std = 0.0067\n",
      "random_forest: Mean accuracy = 0.5951, Std = 0.0072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\"\"\"\n",
    "This script performs feature selection and model evaluation using various classifiers with a pipeline approach.\n",
    "\n",
    "The script includes the following steps:\n",
    "1. Import necessary libraries and modules.\n",
    "2. Define the number of features to select using SelectKBest.\n",
    "3. Create a dictionary of classifiers, each with a pipeline that includes feature selection and the classifier.\n",
    "4. Perform 5-fold cross-validation for each model.\n",
    "5. Print the cross-validation results, including mean accuracy and standard deviation for each model.\n",
    "\n",
    "Classifiers included:\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Classifier (SVC)\n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "- Polynomial LDA\n",
    "- Polynomial Logistic Regression\n",
    "- Gaussian Naive Bayes (GNB)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Radial Basis Function (RBF) SVC\n",
    "- Random Forest\n",
    "\n",
    "Parameters:\n",
    "- k (int): The number of features to select using SelectKBest.\n",
    "\n",
    "Possible values for the parameter \"scoring\" in cross_val_score:\n",
    "- 'accuracy'\n",
    "- 'adjusted_rand_score'\n",
    "- 'average_precision'\n",
    "- 'balanced_accuracy'\n",
    "- 'brier_score_loss'\n",
    "- 'f1'\n",
    "- 'f1_micro'\n",
    "- 'f1_macro'\n",
    "- 'f1_weighted'\n",
    "- 'f1_samples'\n",
    "- 'neg_log_loss'\n",
    "- 'precision'\n",
    "- 'recall'\n",
    "- 'roc_auc'\n",
    "- 'roc_auc_ovr'\n",
    "- 'roc_auc_ovo'\n",
    "- 'roc_auc_ovr_weighted'\n",
    "- 'roc_auc_ovo_weighted'\n",
    "- 'neg_mean_absolute_error'\n",
    "- 'neg_mean_squared_error'\n",
    "- 'neg_root_mean_squared_error'\n",
    "- 'r2'\n",
    "- 'explained_variance'\n",
    "- 'max_error'\n",
    "- 'neg_median_absolute_error'\n",
    "- 'neg_mean_poisson_deviance'\n",
    "- 'neg_mean_gamma_deviance'\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the number of features to select\n",
    "k = 10\n",
    "\n",
    "\n",
    "# Update classifiers to include feature selection\n",
    "classifiers = {\n",
    "    'lda': Pipeline([('select', SelectKBest(f_classif, k=k)), ('lda', LinearDiscriminantAnalysis())]),\n",
    "    'log_reg': Pipeline([('select', SelectKBest(f_classif, k=k)), ('log_reg', LogisticRegression(max_iter=100000, random_state=42))]),\n",
    "    'svc_linear': Pipeline([('select', SelectKBest(f_classif, k=k)), ('svc_linear', LinearSVC(dual='auto'))]),\n",
    "    'qda': Pipeline([('select', SelectKBest(f_classif, k=k)), ('qda', QuadraticDiscriminantAnalysis())]),\n",
    "    'lda_poly': Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures(2)), ('select', SelectKBest(f_classif, k=k)), ('lda', LinearDiscriminantAnalysis())]),\n",
    "    'log_reg_poly': Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures(2)), ('select', SelectKBest(f_classif, k=k)), ('log_reg', LogisticRegression(penalty=None, max_iter=100000))]),\n",
    "    'gnb': Pipeline([('select', SelectKBest(f_classif, k=k)), ('gnb', GaussianNB())]),\n",
    "    'knn': Pipeline([('scale', StandardScaler()), ('select', SelectKBest(f_classif, k=k)), ('knn', KNeighborsClassifier())]),\n",
    "    'svc_rbf': Pipeline([('scale', StandardScaler()), ('select', SelectKBest(f_classif, k=k)), ('svc', SVC(kernel='rbf'))]),\n",
    "    'random_forest': Pipeline([('select', SelectKBest(f_classif, k=k)), ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation for each model\n",
    "cv_results = {}\n",
    "for model_name, model in classifiers.items():\n",
    "    scores = cross_val_score(model, \n",
    "                             X_train[course_SEM], \n",
    "                             y_train, \n",
    "                             cv=5,\n",
    "                             scoring='accuracy',\n",
    "                             verbose=0,\n",
    "                             n_jobs=-1)\n",
    "    cv_results[model_name] = scores\n",
    "    # print(f\"{model_name}: Mean accuracy = {scores.mean():.4f}, Std = {scores.std():.4f}\")\n",
    "\n",
    "# Print the cross-validation results\n",
    "for model_name, scores in cv_results.items():\n",
    "    print(f\"{model_name}: Mean accuracy = {scores.mean():.4f}, Std = {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda: Mean accuracy = 0.5902, Std = 0.0147\n",
      "log_reg: Mean accuracy = 0.5900, Std = 0.0147\n",
      "svc_linear: Mean accuracy = 0.5901, Std = 0.0147\n",
      "qda: Mean accuracy = 0.5505, Std = 0.0183\n",
      "lda_poly: Mean accuracy = 0.5942, Std = 0.0197\n",
      "log_reg_poly: Mean accuracy = 0.5946, Std = 0.0194\n",
      "gnb: Mean accuracy = 0.5483, Std = 0.0208\n",
      "knn: Mean accuracy = 0.5566, Std = 0.0080\n",
      "svc_rbf: Mean accuracy = 0.5886, Std = 0.0129\n",
      "random_forest: Mean accuracy = 0.5871, Std = 0.0115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\"\"\"\n",
    "This script performs feature selection and model evaluation using various classifiers with a pipeline approach.\n",
    "\n",
    "The script includes the following steps:\n",
    "1. Import necessary libraries and modules.\n",
    "2. Define the number of features to select using SelectKBest.\n",
    "3. Create a dictionary of classifiers, each with a pipeline that includes feature selection and the classifier.\n",
    "4. Perform 5-fold cross-validation for each model.\n",
    "5. Print the cross-validation results, including mean accuracy and standard deviation for each model.\n",
    "\n",
    "Classifiers included:\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Logistic Regression\n",
    "- Linear Support Vector Classifier (SVC)\n",
    "- Quadratic Discriminant Analysis (QDA)\n",
    "- Polynomial LDA\n",
    "- Polynomial Logistic Regression\n",
    "- Gaussian Naive Bayes (GNB)\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Radial Basis Function (RBF) SVC\n",
    "- Random Forest\n",
    "\n",
    "Parameters:\n",
    "- k (int): The number of features to select using SelectKBest.\n",
    "\n",
    "Possible values for the parameter \"scoring\" in cross_val_score:\n",
    "- 'accuracy'\n",
    "- 'adjusted_rand_score'\n",
    "- 'average_precision'\n",
    "- 'balanced_accuracy'\n",
    "- 'brier_score_loss'\n",
    "- 'f1'\n",
    "- 'f1_micro'\n",
    "- 'f1_macro'\n",
    "- 'f1_weighted'\n",
    "- 'f1_samples'\n",
    "- 'neg_log_loss'\n",
    "- 'precision'\n",
    "- 'recall'\n",
    "- 'roc_auc'\n",
    "- 'roc_auc_ovr'\n",
    "- 'roc_auc_ovo'\n",
    "- 'roc_auc_ovr_weighted'\n",
    "- 'roc_auc_ovo_weighted'\n",
    "- 'neg_mean_absolute_error'\n",
    "- 'neg_mean_squared_error'\n",
    "- 'neg_root_mean_squared_error'\n",
    "- 'r2'\n",
    "- 'explained_variance'\n",
    "- 'max_error'\n",
    "- 'neg_median_absolute_error'\n",
    "- 'neg_mean_poisson_deviance'\n",
    "- 'neg_mean_gamma_deviance'\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the number of features to select\n",
    "k = 10\n",
    "\n",
    "\n",
    "# Update classifiers to include feature selection\n",
    "classifiers = {\n",
    "    'lda': Pipeline([('select', SelectKBest(f_classif, k=k)), ('lda', LinearDiscriminantAnalysis())]),\n",
    "    'log_reg': Pipeline([('select', SelectKBest(f_classif, k=k)), ('log_reg', LogisticRegression(max_iter=100000, random_state=42))]),\n",
    "    'svc_linear': Pipeline([('select', SelectKBest(f_classif, k=k)), ('svc_linear', LinearSVC(dual='auto'))]),\n",
    "    'qda': Pipeline([('select', SelectKBest(f_classif, k=k)), ('qda', QuadraticDiscriminantAnalysis())]),\n",
    "    'lda_poly': Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures(2)), ('select', SelectKBest(f_classif, k=k)), ('lda', LinearDiscriminantAnalysis())]),\n",
    "    'log_reg_poly': Pipeline([('scale', StandardScaler()), ('poly', PolynomialFeatures(2)), ('select', SelectKBest(f_classif, k=k)), ('log_reg', LogisticRegression(penalty=None, max_iter=100000))]),\n",
    "    'gnb': Pipeline([('select', SelectKBest(f_classif, k=k)), ('gnb', GaussianNB())]),\n",
    "    'knn': Pipeline([('scale', StandardScaler()), ('select', SelectKBest(f_classif, k=k)), ('knn', KNeighborsClassifier())]),\n",
    "    'svc_rbf': Pipeline([('scale', StandardScaler()), ('select', SelectKBest(f_classif, k=k)), ('svc', SVC(kernel='rbf'))]),\n",
    "    'random_forest': Pipeline([('select', SelectKBest(f_classif, k=k)), ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
    "}\n",
    "\n",
    "# Perform 5-fold cross-validation for each model\n",
    "cv_results = {}\n",
    "for model_name, model in classifiers.items():\n",
    "    scores = cross_val_score(model, \n",
    "                             X_train, \n",
    "                             y_train, \n",
    "                             cv=5,\n",
    "                             scoring='accuracy',\n",
    "                             verbose=0,\n",
    "                             n_jobs=-1)\n",
    "    cv_results[model_name] = scores\n",
    "    # print(f\"{model_name}: Mean accuracy = {scores.mean():.4f}, Std = {scores.std():.4f}\")\n",
    "\n",
    "# Print the cross-validation results\n",
    "for model_name, scores in cv_results.items():\n",
    "    print(f\"{model_name}: Mean accuracy = {scores.mean():.4f}, Std = {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.8, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 6, 'learning_rate': 0.1, 'lambda': 1.5, 'gamma': 0.2, 'eval_metric': 'logloss', 'colsample_bytree': 1.0, 'alpha': 0}\n",
      "Best accuracy found:  0.6575898915424481\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=500, \n",
    "                                   scoring='accuracy', \n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy found: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Best parameters found:  {'subsample': 1.0, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 3, 'learning_rate': 0.2, 'lambda': 1.5, 'gamma': 0, 'eval_metric': 'logloss', 'colsample_bytree': 0.6, 'alpha': 0.1}\n",
      "Best accuracy found:  0.639114172461572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/erdos_fall_2024/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train[courses], label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=500, \n",
    "                                   scoring='accuracy', \n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1)\n",
    "random_search.fit(X_train[courses], y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy found: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "Best parameters found:  {'subsample': 0.8, 'objective': 'binary:logistic', 'n_estimators': 100, 'min_child_weight': 3, 'max_depth': 3, 'learning_rate': 0.2, 'lambda': 2, 'gamma': 0, 'eval_metric': 'logloss', 'colsample_bytree': 1.0, 'alpha': 0.2}\n",
      "Best accuracy found:  0.6444248344069906\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 10],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'lambda': [1, 1.5, 2],\n",
    "    'alpha': [0, 0.1, 0.2],\n",
    "    'n_estimators': [100],  # Changed from 'num_boost_round' to 'n_estimators'\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss'],\n",
    "}\n",
    "\n",
    "# Create the DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(data=X_train[course_SEM], label=y_train)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=500, \n",
    "                                   scoring='accuracy', \n",
    "                                   verbose=1, \n",
    "                                   n_jobs=-1)\n",
    "random_search.fit(X_train[course_SEM], y_train)\n",
    "\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best accuracy found: \", random_search.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_fall_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
